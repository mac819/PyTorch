{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Linear` model can be seen as a **layer** in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
      "        [ 0.5304, -0.1265,  0.1165],\n",
      "        [-0.2811,  0.3391,  0.5090],\n",
      "        [-0.4236,  0.5018,  0.1081],\n",
      "        [ 0.4266,  0.0782,  0.2784]])), ('0.bias', tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472])), ('1.weight', tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]])), ('1.bias', tensor([-0.2062]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this sequential model does not have attributes names, `state_dict()` uses **numeric prefixes**.\n",
    "\n",
    "You can also use a model's `add_module()` method to be able to name the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layer1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Building the model from the figure above\n",
    "model = nn.Sequential()\n",
    "model.add_module('layer1', nn.Linear(3, 5))\n",
    "model.add_module('layer2', nn.Linear(5, 1))\n",
    "print(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv1d layer: `nn.Conv1d()` applies 1D convolution over the input. `nn.Conv1d()` expects the input to be of the shape `[batch_size, input_channels, signal_length]`\n",
    "\n",
    "**Required Parameters**\n",
    "- `in_channels`: Number of channels in input data \n",
    "- `out_channels`: Number of channels produced after convolution\n",
    "- `kernel_size`: Size of convolving kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float)\n",
    "input_1d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the shape of input tensor so that it matches with dimension of `batch_size=1`, `input_channels=1` and `signal_length=10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d = input_1d.view(1, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping `output_channels=1`, `kernel_size=3` and `stride=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "cnn1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "out_1d = cnn1d(input_1d)\n",
    "print(out_1d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/Users/mayankanand/Documents/pytorch/images/conv_operation.png\" alt=\"Convolution Operation Formula\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "signal_2d = torch.randn(2, 2, 10, dtype=torch.float)\n",
    "print(signal_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a convolution 1-d layer with `out_channels=1`, `kernel_size=3`, `stride=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "conv1d = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=1)\n",
    "out_signal_2d = conv1d(signal_2d)\n",
    "print(out_signal_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying convolution for `out_channels=5`, `kernel_size=3` and `stride=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "conv1d = nn.Conv1d(in_channels=2, out_channels=5, kernel_size=3, stride=1)\n",
    "out_signal_2d = conv1d(signal_2d)\n",
    "print(out_signal_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conv2d layer**: `nn.Conv2d()` applies convolution over the input. `nn.Conv2d()` expects the input to be of the shape `[batch_size, input_channels, input_height, input_width]`.\n",
    "\n",
    "**Required Parameters**:\n",
    "- `in_channels`: Number of channels in the 2d input eg. image.\n",
    "- `out_channels`: Number of channels produced by the convolution\n",
    "- `kernel_size`: size of the convolving layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is generating image with batch size of 3, input_channel of 3, height=25 and width=25\n",
    "input_img_2d = torch.randn(3, 3, 25, 25, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying convolution with `in_channels=3`, `out_channels=1`, `kernel_size=3`, `stride=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 23, 23])\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1)\n",
    "output_img_2d = conv2d(input_img_2d)\n",
    "print(output_img_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling Layers\n",
    "\n",
    "Pooling is commonly used in Convolution Neural Networks (CNNs), It is used to downsample the input data, reducing it's size while retaining it's important features. There are few types of pooling layers, but among them two popular one are:\n",
    "\n",
    "- **Max Pooling**: In Max pooling layer, the kernel computes maximum value from the input data while it convolve and maps it to feature map.\n",
    "- **Average Pooling**: In Average pooling layer, the kernel computes average and maps to feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# 1D signal input\n",
    "signal_1D = torch.randn(3, 2, 10) # Here arguments are, batch_size=3, input_channels=2 and signal_length=10\n",
    "print(signal_1D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool_1d = nn.MaxPool1d(kernel_size=3)\n",
    "pooled_signal_1d = max_pool_1d(signal_1D)\n",
    "pooled_signal_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5328,  2.7864,  0.7368],\n",
       "         [-0.8156,  0.4567,  0.9101]],\n",
       "\n",
       "        [[ 1.1840,  1.4583,  0.4836],\n",
       "         [ 2.2199, -0.0557, -0.1635]],\n",
       "\n",
       "        [[ 0.3526,  2.2495,  2.0145],\n",
       "         [ 0.7511,  0.9250,  0.8122]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_signal_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "image_2d = torch.randn(5, 3, 25, 25) # batch_size=5, input_channels=3, height=25 and width=25\n",
    "print(image_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "maxpool_2d = nn.MaxPool2d(kernel_size=3)\n",
    "output_img_2d = maxpool_2d(image_2d)\n",
    "print(output_img_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise average pooling will also work, instead of taking maximum out of from kernel window, it will compute average and map it to feature map."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding layers\n",
    "\n",
    "Padding layers are used to pad the input sequence along the dimensions. For example in 1-dimension, layer will add values along the signal length of input data on both sides."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `nn.ReflectionPad1d`\n",
    "- Performs reflection padding. Reflection padding mirrors the input tensor at the boundaries, effectively creating a reflection of the input data. This can be useful when you want to maintain the overall structure of the data while avoiding the introduction of artificial values during padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4])\n",
      "torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(8, dtype=torch.float).view(1, 2, 4)\n",
    "print(input.shape)\n",
    "ref_padding_1d = nn.ReflectionPad1d(2)\n",
    "output = ref_padding_1d(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1., 2., 3.],\n",
       "          [4., 5., 6., 7.]]]),\n",
       " tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n",
       "          [6., 5., 4., 5., 6., 7., 6., 5.]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to padd the input data with different lengths on each side then, we can provide information while defining the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 2., 1., 0., 1., 2., 3., 2., 1.],\n",
      "         [7., 6., 5., 4., 5., 6., 7., 6., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "ref_padding_1d = nn.ReflectionPad1d((3, 2))\n",
    "output = ref_padding_1d(input)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `nn.ReflectionPad2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n"
     ]
    }
   ],
   "source": [
    "# input image\n",
    "input = torch.arange(120).view(2, 3, 4, 5)\n",
    "print(input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[12, 11, 10, 11, 12, 13, 14, 13, 12],\n",
      "         [ 7,  6,  5,  6,  7,  8,  9,  8,  7],\n",
      "         [ 2,  1,  0,  1,  2,  3,  4,  3,  2],\n",
      "         [ 7,  6,  5,  6,  7,  8,  9,  8,  7],\n",
      "         [12, 11, 10, 11, 12, 13, 14, 13, 12],\n",
      "         [17, 16, 15, 16, 17, 18, 19, 18, 17],\n",
      "         [12, 11, 10, 11, 12, 13, 14, 13, 12],\n",
      "         [ 7,  6,  5,  6,  7,  8,  9,  8,  7]],\n",
      "\n",
      "        [[32, 31, 30, 31, 32, 33, 34, 33, 32],\n",
      "         [27, 26, 25, 26, 27, 28, 29, 28, 27],\n",
      "         [22, 21, 20, 21, 22, 23, 24, 23, 22],\n",
      "         [27, 26, 25, 26, 27, 28, 29, 28, 27],\n",
      "         [32, 31, 30, 31, 32, 33, 34, 33, 32],\n",
      "         [37, 36, 35, 36, 37, 38, 39, 38, 37],\n",
      "         [32, 31, 30, 31, 32, 33, 34, 33, 32],\n",
      "         [27, 26, 25, 26, 27, 28, 29, 28, 27]],\n",
      "\n",
      "        [[52, 51, 50, 51, 52, 53, 54, 53, 52],\n",
      "         [47, 46, 45, 46, 47, 48, 49, 48, 47],\n",
      "         [42, 41, 40, 41, 42, 43, 44, 43, 42],\n",
      "         [47, 46, 45, 46, 47, 48, 49, 48, 47],\n",
      "         [52, 51, 50, 51, 52, 53, 54, 53, 52],\n",
      "         [57, 56, 55, 56, 57, 58, 59, 58, 57],\n",
      "         [52, 51, 50, 51, 52, 53, 54, 53, 52],\n",
      "         [47, 46, 45, 46, 47, 48, 49, 48, 47]]])\n"
     ]
    }
   ],
   "source": [
    "ref_padding_2d = nn.ReflectionPad2d(2)\n",
    "output = ref_padding_2d(input)\n",
    "print(output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example we can observe that numbers are added on both dimension of height and width on the image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likewise zero padding will also work, instead of adding numbers with considering boundary values are mirror, it will add padding values as zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
