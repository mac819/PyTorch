{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensors in PyTorch have equivalent functions as its Numpy counterpart like: `ones()`, `zeros()`, `rand()`, `randn()`, and many more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[-0.4561, -0.6138, -1.1237,  0.6508],\n",
      "         [ 0.2088,  1.0292,  1.1256,  0.7728],\n",
      "         [ 0.9694, -0.2683, -0.1075, -2.3003]],\n",
      "\n",
      "        [[ 0.5485, -0.8970,  1.8781,  0.1102],\n",
      "         [-0.4681, -0.3430, -0.3773,  1.0900],\n",
      "         [ 1.9485,  1.4436, -1.0792,  1.0004]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# creating a scalar, and three tensors (vector, matrix, tensor)\n",
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get it's dimension using it's `size()` method or `size` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# creating a tensor and printing it's shape\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All tensors have it's dimension except scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# creating a scalar and printing it's shape\n",
    "scalar = torch.tensor(3.14159)\n",
    "print(scalar.size(), scalar.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also reshape your tensors using `view()` (preferred) method or `reshape()` methods\n",
    "\n",
    "**Beware**: \n",
    "- The `view()` method only returns a tensor with desired shape that shares the underlying data with the original tensor. It doesn't create a new, independent one. \\\n",
    "- The `reshape()` may or maynot create a copy! That's why `view()` is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "# We get a tensor with a different shape but it still is the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print(same_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to copy or duplicate all the data for real. Then you can use either it's `new_tensor()` or `clone()` mothods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/3m09y2cx0cq_1x4gp8q0ng2m0000gn/T/ipykernel_53794/439821682.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  different_matrix = matrix.new_tensor(matrix.view(1, 6))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "# We can use \"new_tensor\" method to REALLY copy it  into a new one\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\n",
    "# Now, if we change one of its elements...\n",
    "different_matrix[0, 1] = 3.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "# But we get a \"warning\" from PyTorch telling us \n",
    "# to use \"clone()\" instead!\n",
    "print(matrix)\n",
    "print(different_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code you might be getting UserWarning due to `new_tensor()` method. It seems PyTorch prefers that we use `clone()` instead of `new_tensor()`. Both way accompolish the same result, but code below is deemed cleaner and more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print(another_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: What about the `detach()` method? What does it do?\n",
    "- A: It removes the tensor from computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data, Devices and CUDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion between Numpy and PyTorch "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `as_tensor()` method preserves the type of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_array = np.array([2, 1, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Using as_tensor function to convert Numpy array to PyTorch tensor\n",
    "dummy_array_tensor = torch.as_tensor(dummy_array)\n",
    "print(dummy_array.dtype, dummy_array_tensor.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also cast it to a different type like lower presicion (32-bit) which will take even lesser memory using `float()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Using float function to change data types \n",
    "dummy_array_tensor = torch.as_tensor(dummy_array)\n",
    "float_tensor = dummy_array_tensor.float() # (going from float64 to float32) \n",
    "print(float_tensor.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Important**: Both `as_tensor()` and `from_numpy()` return a tensor that shares the underlying data with the original Numpy array. Similar to what happened when we used `view()` in the previous lesson, if you modify the original Numpy array, you are modifying the corresponding PyTorch tensor too and vice-versa.\n",
    "\n",
    "- Q: What do we need `as_tensor()` for? Why can’t we just use `torch.tensor()`?\n",
    "- A: Well, you could. Just keep in mind that `torch.tensor()` always makes a copy of the data instead of sharing the underlying data with the Numpy array.\n",
    "\n",
    "You can also perform the opposite operation; namely, transforming a PyTorch tensor back to a Numpy array using the `numpy()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Using numpy function to convert PyTorch tensor to Numpy array\n",
    "print(dummy_tensor.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use `cuda.is_available()` to find out if you have a GPU at your disposal and set your device accordingly. So, it is good practice to figure this out at the top of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if your device has GPU or not\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are using someone else’s computer and you do not know how many GPUs it has or which model they are, you can figure it out using cuda.`device_count()` and `cuda.get_device_name()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_cudas = torch.cuda.device_count()\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Turning tensor into GPU tensor:\n",
    "- There is only one thing left to do; turn our tensor into a GPU tensor. That is what `to()` is good for. It sends a tensor to the specified device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# converting the training numpy array to a tensor first and then to a GPU tensor\n",
    "gpu_tensor = torch.as_tensor(dummy_array).to(device)\n",
    "print(gpu_tensor[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For GPU, expected output would be: `tensor(1, device='cuda:0', dtype=torch.float64)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What distinguishes a tensor used for training data from a tensor used as a trainable parameter/weight?\n",
    "\n",
    "The latter requires the computation of its gradients, so we can update their values (the parameters’ values, that is). That is what the `requires_grad=True` argument is good for. It tells PyTorch to compute gradients for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Parameters only for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# FIRST\n",
    "# Initializes parameters \"b\" and \"w\" randomly, ALMOST as we \n",
    "# did in Numpy since we want to apply gradient descent on \n",
    "# these parameters we need to set REQUIRES_GRAD = TRUE\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating trainable parameters for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just \n",
    "# send them to device, right?\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(b, w)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU users will get an output similat to the following:\n",
    "    - `tensor([0.3367], device:'cuda:0', grad_fn=<CopyBackwards>)`\n",
    "    - `tensor([0.1288], device:'cuda:0', grad_fb=<CopyBackwards>)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We succeeded in sending them to another device, but we lost the gradients somehow since there is no more `requires_grad=True` (don’t bother with the weird grad_fn). Clearly, we need to do better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Attempt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this attempt, we first send the tensor to GPU and then set it's `required_gradient=True`\n",
    "- In PyTorch, every method that ends with an underscore ( _ ) like the requires_grad_() method above, makes changes in-place; meaning, they will modify the underlying variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# THIRD\n",
    "# We can either create regular tensors and send them to \n",
    "# the device (as we did with our data)\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "b.requires_grad_()\n",
    "w.requires_grad_()\n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU users will get an output similat to the following:\n",
    "    - `tensor([0.3367], device:'cuda:0', requires_grad=True)`\n",
    "    - `tensor([0.1288], device:'cuda:0', required_grad=True)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Attempt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yes, we can do better. We can assign tensors to a device at the moment of their creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# FINAL\n",
    "# We can specify the device at the moment of creation\n",
    "# RECOMMENDED!\n",
    "\n",
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU users will get an output similat to the following:\n",
    "    - `tensor([0.3367], device:'cuda:0', requires_grad=True)`\n",
    "    - `tensor([0.1288], device:'cuda:0', required_grad=True)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Always assign tensors to a device at the moment of its creation to avoid unexpected behaviors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd in PyTorch's automatic defferential package. Thanks to it, we do not need to worry about partial derivatives, chain rules, or anything like it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `backward` method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While doing forward propogation in a model, we get final loss value (using error expression). We have set `requires_grad=True` to both `b` and `w`, so they are obviously included in the list of trainable parameters. Then we use `yhat` to compute the `error`, which is also added to the list. Hence, the following will be handled by the backward method:\n",
    "    - `b`\n",
    "    - `w`\n",
    "    - `yhat`\n",
    "    - `error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor([2, 1, 3, 5])\n",
    "y_train_tensor = torch.tensor([2, 4, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - computes the loss\n",
    "# We are using ALL data points, so this is BATCH gradient \n",
    "# descent. How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - computes gradients for both \"b\" and \"w\" parameters\n",
    "# No more manual computation of gradients! \n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_tensor * error).mean()   \n",
    "loss.backward()                           # 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `error`, `yhat`, `b` and `w` have `requires_grad=True` where as `x_train_tensor` and `y_train_tensor` doesn't has `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "# using requires_grad to check if the following tensor require gradients or not\n",
    "print(error.requires_grad, yhat.requires_grad, \\\n",
    "      b.requires_grad, w.requires_grad)\n",
    "print(y_train_tensor.requires_grad, x_train_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-13.8545]) tensor([-28.9093])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad, w.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat the `backward` method multiple times, then `grad` value of trainable paramers starts accumulate. This is not what we want while training a model. So to avoid that, we need to set the grad back to zero using `zero_()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad.zero_(), w.grad.zero_())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below code there are 3 attempts to create a traning loop with `n_epochs` number of epochs. In below case if we get grad of a trainable parameters using `grad` attribute, then after a call it assigns `None` to trainable parameter. So this throws `UserWarning` in first case. In second case PyTorch is unable to do dymanic computation. So the best approach is to use with context of `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.7035], requires_grad=True) tensor([-29.9299], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 40\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient \n",
    "    # descent. How wrong is our model? That's the error! \n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - computes gradients for both \"b\" and \"w\"\n",
    "    # parameters. No more manual computation of gradients! \n",
    "    # b_grad = 2 * error.mean()\n",
    "    # w_grad = 2 * (x_tensor * error).mean()   \n",
    "    # We just tell PyTorch to work its way BACKWARDS\n",
    "    # from the specified loss!\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters using gradients and\n",
    "    # the learning rate. But not so fast...\n",
    "    # FIRST ATTEMPT - just using the same code as before\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # b = b - lr * b.grad                          # 1)\n",
    "    # w = w - lr * w.grad                          # 1)\n",
    "    # print(b)                                     # 1)\n",
    "\n",
    "    # SECOND ATTEMPT - using in-place Python assingment\n",
    "    # RuntimeError: a leaf Variable that requires grad\n",
    "    # has been used in an in-place operation.\n",
    "    # b -= lr * b.grad                             # 2)\n",
    "    # w -= lr * w.grad                             # 2)\n",
    "    \n",
    "    # THIRD ATTEMPT - NO_GRAD for the win!\n",
    "    # We need to use NO_GRAD to keep the update out of \n",
    "    # the gradient computation. Why is that? It boils \n",
    "    # down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():                          # 3)\n",
    "        b -= lr * b.grad                           # 3)\n",
    "        w -= lr * w.grad                           # 3)\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we\n",
    "    # need to tell it to let it go...\n",
    "    b.grad.zero_()                                 # 4)\n",
    "    w.grad.zero_()                                 # 4)\n",
    "    \n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n -->\n<!-- Pages: 1 -->\n<svg width=\"220pt\" height=\"284pt\"\n viewBox=\"0.00 0.00 220.00 284.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 280.25)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-280.25 216,-280.25 216,4 -4,4\"/>\n<!-- 5424204752 -->\n<g id=\"node1\" class=\"node\">\n<title>5424204752</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"133,-31.25 79,-31.25 79,0 133,0 133,-31.25\"/>\n<text text-anchor=\"middle\" x=\"106\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> (4)</text>\n</g>\n<!-- 5371662880 -->\n<g id=\"node2\" class=\"node\">\n<title>5371662880</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"150,-86.5 62,-86.5 62,-67.25 150,-67.25 150,-86.5\"/>\n<text text-anchor=\"middle\" x=\"106\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 5371662880&#45;&gt;5424204752 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5371662880&#45;&gt;5424204752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106,-66.88C106,-60.39 106,-51.29 106,-42.62\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-42.71 106,-32.71 102.5,-42.71 109.5,-42.71\"/>\n</g>\n<!-- 5371666240 -->\n<g id=\"node3\" class=\"node\">\n<title>5371666240</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-141.75 0,-141.75 0,-122.5 100,-122.5 100,-141.75\"/>\n<text text-anchor=\"middle\" x=\"50\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5371666240&#45;&gt;5371662880 -->\n<g id=\"edge1\" class=\"edge\">\n<title>5371666240&#45;&gt;5371662880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.5,-122.09C67.51,-114.47 79.18,-103.37 88.83,-94.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.81,-97.2 95.64,-87.77 85.98,-92.13 90.81,-97.2\"/>\n</g>\n<!-- 4374667488 -->\n<g id=\"node4\" class=\"node\">\n<title>4374667488</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-209 23,-209 23,-177.75 77,-177.75 77,-209\"/>\n<text text-anchor=\"middle\" x=\"50\" y=\"-183.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 4374667488&#45;&gt;5371666240 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4374667488&#45;&gt;5371666240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50,-177.42C50,-170.05 50,-161.05 50,-153.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-153.16 50,-143.16 46.5,-153.16 53.5,-153.16\"/>\n</g>\n<!-- 5371110544 -->\n<g id=\"node5\" class=\"node\">\n<title>5371110544</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-141.75 118,-141.75 118,-122.5 206,-122.5 206,-141.75\"/>\n<text text-anchor=\"middle\" x=\"162\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 5371110544&#45;&gt;5371662880 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5371110544&#45;&gt;5371662880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M152.5,-122.09C144.49,-114.47 132.82,-103.37 123.17,-94.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"126.02,-92.13 116.36,-87.77 121.19,-97.2 126.02,-92.13\"/>\n</g>\n<!-- 5371109968 -->\n<g id=\"node6\" class=\"node\">\n<title>5371109968</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"212,-203 112,-203 112,-183.75 212,-183.75 212,-203\"/>\n<text text-anchor=\"middle\" x=\"162\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5371109968&#45;&gt;5371110544 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5371109968&#45;&gt;5371110544</title>\n<path fill=\"none\" stroke=\"black\" d=\"M162,-183.38C162,-175.37 162,-163.38 162,-153.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"165.5,-153.21 162,-143.21 158.5,-153.21 165.5,-153.21\"/>\n</g>\n<!-- 5373008608 -->\n<g id=\"node7\" class=\"node\">\n<title>5373008608</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"189,-276.25 135,-276.25 135,-245 189,-245 189,-276.25\"/>\n<text text-anchor=\"middle\" x=\"162\" y=\"-250.75\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 5373008608&#45;&gt;5371109968 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5373008608&#45;&gt;5371109968</title>\n<path fill=\"none\" stroke=\"black\" d=\"M162,-244.82C162,-235.73 162,-223.98 162,-214.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"165.5,-214.3 162,-204.3 158.5,-214.3 165.5,-214.3\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x14031faf0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1 - computes our model's predicted output - forward pass\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2 - computes the loss\n",
    "error = (yhat - y_train_tensor)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# We can try plotting the graph for any python variable:\n",
    "# yhat, error, loss...\n",
    "make_dot(yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Blue boxes ((1)s)**: These boxes correspond to the tensors we use as parameters; the ones we are asking PyTorch to compute gradients for.\n",
    "- **Gray boxes (MulBackward0 and AddBackward0)**: A python operation that involves a gradient-computing tensor or its dependencies.\n",
    "- **Green box (4)**: The tensor is used as the starting point for the computation of gradients, assuming the `backward()` method is called from the variable used to visualize the graph. They are computed from the bottom-up in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When number of trainable parameters increases, then it becomes difficult to keep track of each parameters. To rescue this, PyTorch provides it's own modules of `optimizer` such as `SGD`, `RMSprop` or `Adam`\n",
    "\n",
    "- There are so many optimizers; SGD is the most basic of them, and Adam is one of the most popular.\n",
    "\n",
    "- Different optimizers use different mechanics for updating the parameters, but they all achieve the same goal through (literally) different paths.\n",
    "\n",
    "- Remember: The choice of mini-batch size influenced the path of gradient descent, and so does the choice of an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "print(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `step` and `zero_grad` methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well), and performs the updates through its `step()` method.\n",
    "\n",
    "Besides, we also do not need to zero the gradients one-by-one anymore. We just invoke the optimizer’s `zero_grad()` method, and that is it!\n",
    "\n",
    "In the code below, we create a **Stochastic Gradient Descent** (SGD) optimizer to update our parameters `b` and `w`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not be fooled by the optimizer’s name. If we use all training data at once for the update as we are actually doing in this code, the optimizer is performing a batch gradient descent despite its name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.7035], requires_grad=True) tensor([-29.9299], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)               # 1)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 40\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - computes the loss\n",
    "    # We are using ALL data points, so this is BATCH gradient\n",
    "    # descent. How wrong is our model? That's the error! \n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3 - computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters using gradients and \n",
    "    # the learning rate. No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()                               # 2)\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()                          # 3)\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing loss of a model, PyTorch provides a variety of loss functions to choose from depending on the task at hand. For example, for regression task, we define a Mean Squared Error (MSE) as our loss. `nn.MSELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "print(loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that nn.MSELoss is not the loss function itself. We do not pass predictions and labels to it! Instead, as you can see, it returns another function, which we called loss_fn. That is the actual loss function. So, we can pass a prediction and a label to it, and get the corresponding loss value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1700)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# This is a random example to illustrate the loss function\n",
    "predictions = torch.tensor([0.5, 1.0])\n",
    "labels = torch.tensor([2.0, 1.3])\n",
    "print(loss_fn(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor([1, 2, 3, 5], dtype=torch.float)\n",
    "y_train_tensor = torch.tensor([2, 4, 4, 1], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.8576], requires_grad=True) tensor([-28.0349], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like\n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, \\\n",
    "                dtype=torch.float, device=device)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')             # 1)\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 40\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2 - computes the loss\n",
    "    # No more manual loss!\n",
    "    # error = (yhat - y_train_tensor)\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)           # 2)\n",
    "\n",
    "    # Step 3 - computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters using gradients and \n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting loss tensor to Numpy array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to have it as a Numpy array? We could just use `numpy()` again right? (And use `cpu()` as well since our loss is in the `cuda` device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What heppened here? Unlike our data tensors, the loss tensor is actually computing gradients. To use `numpy()`, we need to `detach()` the tensor from the computation graph first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7202.655\n"
     ]
    }
   ],
   "source": [
    "# converting to numpy using detach since gradient is being computed\n",
    "print(loss.detach().cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a lot of work, there must be an easier way! We can use item() for tensors with a single element or tolist() otherwise (it still returns a scalar if there is only one element though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7202.65478515625 7202.65478515625\n"
     ]
    }
   ],
   "source": [
    "# using item and tolist methods for converting loss tensor to numpy\n",
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a model is represented by a regular Python class that inherits from the `Module` class. The most fundamental methods a model class needs to implement are:\n",
    "- `__init__(self)`: It defines the parts that make up the model; in our case, two parameters of `b` and `w`.\n",
    "    - You are not limited to defining parameters though. Models can contain other models as their attributes as well, so you can easily nest them.\n",
    "    - Besides, do not forget to include `super().__init__()` to execute the `__init__()` method of the **parent class** (`nn.Module`) before your own.\n",
    "\n",
    "\n",
    "- `forward(self, x)`: It performs the actual computation; that is, it outputs a prediction, given the input `x`\n",
    "    - It may seem weird, but whenever you are using your model to make predictions, you should not call the `forward(x)` method!\n",
    "    - You should call the whole model instead (as `model(x)`) to perform a forward pass and output predictions.\n",
    "    - The reason for this is because the call to the whole model involves extra steps, namely, handling forward and backward hooks. If you do not use hooks (and we do not use any right now), both calls are equivalent.\n",
    "\n",
    "Hooks are a very useful mechanism that allows retrieving intermediate values in deeper models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model, \n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1, \n",
    "                                          requires_grad=True, \n",
    "                                          dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, \n",
    "                                          requires_grad=True,\n",
    "                                          dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Parameter` and `parameters` methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `__init__` method, we define our two paramters of `b` and `w`, using the `Parameter()` class, to tell PyTorch that these tensors, which are attributes of the `ManualLinearRegression` class, should be considered parameters of the model the class represents.\n",
    "\n",
    "Why should we care about that? By doing so, we can use our model's `parameters()` method to retrieve an iterator over all model's parameters, including parameters of **nested models**. Then we can use it to feed our optimizer (instead of building a list of parameters ourselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.3367], requires_grad=True), Parameter containing:\n",
      "tensor([0.1288], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "print(list(dummy.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `state_dict` method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `state_dict()` of a given model is simply a Python dictionary that maps each attributes/parameter to its corresponding tensor. But only learnable parameters are included, as its purpose is to keep track of parameters that are going to be updated by the optimizer.\n",
    "\n",
    "By the way, the **optimizer** itself has a `state_dict()` too, which contains its internal state as well as other hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "# Defines a SGD optimizer\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to send our dummy model to a device, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "# and sends it to the device\n",
    "dummy = ManualLinearRegression().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **forward pass** is the moment when the model **makes predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([-3873.1943])), ('w', tensor([-13453.2734]))])\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like \n",
    "# Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 0 - initializes parameters \"b\" and \"w\" randomly\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)        # 1)\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Defines number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # What is this?                # 2)\n",
    "\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)                   # 3)\n",
    "    \n",
    "    # Step 2 - computes the loss\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3 - computes gradients for both \"b\" and \"w\" parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters using gradients and\n",
    "    # the learning rate\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `train` method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, models have a `train()` method, which does NOT perform a training step. Its only purpose is to set the model to training mode.\n",
    "\n",
    "Why is this important? Some models may use mechanism like `Dropout` for instance, which have distinct behavoprs during training and evaluation phases.\n",
    "\n",
    "It is good practice to call `model.train()` in the training loop. It is also possible to set a model to evaluation mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a model with defining a linear model inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model \n",
    "        # with a single input and a single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we call the `parameters()` method of this model, PyTorch will figure the parameters of this attributes recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8300], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# creating dummy instance of our MyLinearRegression model\n",
    "dummy = MyLinearRegression().to(device)\n",
    "print(list(dummy.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, you can also use state_dict() to get the parameter values together with their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[-0.2343]])), ('linear.bias', tensor([0.9186]))])\n"
     ]
    }
   ],
   "source": [
    "# creating dummy instance of our MyLinearRegression model\n",
    "dummy = MyLinearRegression().to(device)\n",
    "print(dummy.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was simple enough. You may be thinking: “why even bother to build a class for it?” Well, you have a point.\n",
    "\n",
    "For straightforward models that use a series of built-in PyTorch models (like `Linear`) where the output of one is sequentially fed as an input to the next, we can use a `Sequential` model.\n",
    "\n",
    "In our case, we would build a `Sequential` model with a single argument; that is, the `Linear` model we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
